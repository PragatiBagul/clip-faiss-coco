# llama/configs/llama.yaml
# LLAMA / HF model configuration used by inference.py
# Edit model_name to a model you have access to (Llama-2, Vicuna etc.)
# If you want to run on CPU, set device: "cpu"

model_name: "meta-llama/Llama-2-7b"
device: "cpu"
use_8bit: true
adapter_path: null
max_length: 256
temperature: 0.7
top_p: 0.9
num_return_sequences: 1
seed: 42

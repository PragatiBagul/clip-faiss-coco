# LoRA hyperparameter recommendations
lora_r: 8
lora_alpha: 32
lora_dropout: 0.1
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - dense
training:
  batch_size: 32
  lr: 2e-4
  warmup: 200
  epochs: 3

# app/routers/generation.py
"""
Generation router:
- /generate/caption_from_image  (multipart file OR image_id)
- /generate/explanation_from_text_query (JSON)

This router depends on the global `state` object from app.main:
state must provide: model (CLIP), preprocess, index (FAISS), metadata, device, clip_config

It also uses:
- llama.inference.inference.generate_from_prompt
- llama.inference.prompt_builder.build_caption_prompt / build_explanation_prompt
- llama.inference.context_builder.build_context_from_clip_results
"""

import io
import json
import re
import base64
from typing import Optional
from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import numpy as np
import torch
from PIL import Image

from app.schemas.generation_schemas import (
    CaptionFromImageRequest,
    CaptionFromImageResponse,
    ExplanationRequest,
    ExplanationResponse,
    RetrievedItem
)

from app.main import state  # your app.main must expose 'state'
from llama.inference import prompt_builder, inference as llama_inf
from llama.inference.context_builder import build_context_from_clip_results

router = APIRouter()


def _parse_json_from_model_output(generated_text: str):
    """
    Best-effort: extract the first JSON object from generated text.
    If not found, return {'raw': generated_text}.
    """
    try:
        m = re.search(r"\{.*\}", generated_text, re.S)
        if m:
            return json.loads(m.group(0))
    except Exception:
        pass
    return {"raw": generated_text}


@router.post("/generate/caption_from_image", response_model=CaptionFromImageResponse)
async def generate_caption_from_image(
    file: Optional[UploadFile] = File(None),
    image_id: Optional[int] = None,
    k: int = 8,
    max_new_tokens: int = 128,
    temperature: float = 0.7,
    top_p: float = 0.9,
    seed: Optional[int] = None
):
    """
    Upload an image or pass image_id (existing dataset).
    Returns structured caption generated by LLaMA using retrieved captions as evidence.
    """
    if state.index is None:
        raise HTTPException(status_code=503, detail="FAISS index not loaded")

    if file is None and image_id is None:
        raise HTTPException(status_code=400, detail="Provide file or image_id")

    # 1) Obtain CLIP embedding
    if file is not None:
        body = await file.read()
        try:
            img = Image.open(io.BytesIO(body)).convert("RGB")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Invalid image: {e}")

        # preprocess using state.preprocess (same used for embedding building)
        tensor = state.preprocess.image_transform(img).unsqueeze(0).to(state.device)
        with torch.no_grad():
            emb = state.model.encode_image(tensor).cpu().numpy().astype(np.float32)
    else:
        # image_id provided: attempt to locate file_path in metadata
        try:
            meta = next((m for m in state.metadata if int(m.get("image_id", -1)) == int(image_id)), None)
        except Exception:
            meta = None
        if meta is None:
            raise HTTPException(status_code=404, detail="image_id not found in metadata")
        # load image and encode
        try:
            img = Image.open(meta["file_path"]).convert("RGB")
            tensor = state.preprocess.image_transform(img).unsqueeze(0).to(state.device)
            with torch.no_grad():
                emb = state.model.encode_image(tensor).cpu().numpy().astype(np.float32)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Could not load/encode image for image_id={image_id}: {e}")

    # normalize if necessary
    if state.clip_config.get("normalize_embeddings", True):
        norms = np.linalg.norm(emb, axis=1, keepdims=True)
        norms[norms == 0.0] = 1.0
        emb = emb / norms

    # 2) FAISS search
    k = int(k or 8)
    D, I = state.index.search(emb, k)
    I = I[0].tolist()
    D = D[0].tolist()

    # 3) map to metadata
    retrieved = []
    for rank, (idx, score) in enumerate(zip(I, D), start=1):
        if idx < 0 or idx >= len(state.metadata):
            continue
        meta = state.metadata[idx]
        retrieved.append({
            "rank": rank,
            "image_path": meta.get("file_path"),
            "caption": meta.get("caption"),
            "score": float(score),
            # optional metadata fields can be present: width/height/image_id
            "image_id": meta.get("image_id")
        })

    # 4) build context (dedupe/truncate) and prompt
    ctx = build_context_from_clip_results(retrieved, max_items=k, tokenizer=None, max_tokens=None, include_visual=False)
    items = ctx["items"]
    prompt = prompt_builder.build_caption_prompt(items, max_items=k)

    # 5) call LLaMA
    llama_inf.load_model()  # lazy
    generated = llama_inf.generate_from_prompt(
        prompt,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        num_return_sequences=1,
        seed=seed
    )

    output = _parse_json_from_model_output(generated)

    # build response objects
    retrieved_objs = [RetrievedItem(**{
        "rank": r["rank"],
        "image_path": r.get("image_path"),
        "caption": r.get("caption"),
        "score": r.get("score"),
        "visual": r.get("visual", "")
    }) for r in items]

    return {
        "query_type": "image",
        "image_id": int(image_id) if image_id is not None else None,
        "retrieved": retrieved_objs,
        "prompt": prompt,
        "output": output
    }


@router.post("/generate/explanation_from_text_query", response_model=ExplanationResponse)
async def generate_explanation_from_text(req: ExplanationRequest):
    """
    Given a text query, retrieve images and generate explanation/summarization.
    """
    if state.index is None:
        raise HTTPException(status_code=503, detail="FAISS index not loaded")

    q = req.query.strip()
    if not q:
        raise HTTPException(status_code=400, detail="Empty query")

    # encode text
    with torch.no_grad():
        tokens = state.model.tokenize([q]).to(state.device)
        txt_emb = state.model.encode_text(tokens).cpu().numpy().astype(np.float32)

    if state.clip_config.get("normalize_embeddings", True):
        norms = np.linalg.norm(txt_emb, axis=1, keepdims=True)
        norms[norms == 0.0] = 1.0
        txt_emb = txt_emb / norms

    D, I = state.index.search(txt_emb, int(req.k or 8))
    I = I[0].tolist()
    D = D[0].tolist()

    retrieved = []
    for rank, (idx, score) in enumerate(zip(I, D), start=1):
        if idx < 0 or idx >= len(state.metadata):
            continue
        meta = state.metadata[idx]
        retrieved.append({
            "rank": rank,
            "image_path": meta.get("file_path"),
            "caption": meta.get("caption"),
            "score": float(score),
            "image_id": meta.get("image_id")
        })

    # build context & prompt
    ctx = build_context_from_clip_results(retrieved, max_items=int(req.k or 8), include_visual=False)
    items = ctx["items"]
    prompt = prompt_builder.build_explanation_prompt(q, items, max_items=int(req.k or 8))

    # generate
    llama_inf.load_model()
    generated = llama_inf.generate_from_prompt(
        prompt,
        max_new_tokens=req.max_new_tokens or 128,
        temperature=req.temperature or 0.7,
        top_p=req.top_p or 0.9,
        num_return_sequences=1,
        seed=req.seed
    )

    output = _parse_json_from_model_output(generated)

    retrieved_objs = [RetrievedItem(**{
        "rank": r["rank"],
        "image_path": r.get("image_path"),
        "caption": r.get("caption"),
        "score": r.get("score"),
        "visual": r.get("visual", "")
    }) for r in items]

    return {
        "query_type": "text",
        "query": q,
        "retrieved": retrieved_objs,
        "prompt": prompt,
        "output": output
    }
